{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for the Simple pendulum inverse equilibrium\n",
    "\n",
    "In theory it could be solved by PID, but let's use big weapon to solve these problem\n",
    "\n",
    "## Environment configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "from function.Dynamics_modeling import *\n",
    "from function.Euler_lagrange import *\n",
    "from function.Render import *\n",
    "from function.Catalog_gen import *\n",
    "\n",
    "from function.ray_env_creator import *\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "import ray\n",
    "from ray import tune,train\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "\n",
    "import pprint\n",
    "\n",
    "# Single pendulum exclusive.....\n",
    "\n",
    "# Initialisation du modèle théorique\n",
    "\n",
    "t = sp.symbols(\"t\")\n",
    "\n",
    "CoordNumb = 1\n",
    "\n",
    "Symb = Symbol_Matrix_g(CoordNumb,t)\n",
    "\n",
    "theta = Symb[1,0]\n",
    "theta_d = Symb[2,0]\n",
    "theta_dd = Symb[3,0]\n",
    "\n",
    "m, l, g = sp.symbols(\"m l g\")\n",
    "\n",
    "L = 0.2\n",
    "Substitution = {\"g\": 9.81, \"l\": L, \"m\": 0.1}\n",
    "\n",
    "Time_end = 14\n",
    "\n",
    "#----------------External Forces--------------------\n",
    "\n",
    "F_ext_time = np.array([0, 2, 4, 6, 8, Time_end])\n",
    "F_ext_Value = np.array([[0, 1, -1, 1, 1, -1]]) * 0.0  # De la forme (k,...)\n",
    "\n",
    "F_ext_func = interpolate.CubicSpline(F_ext_time, F_ext_Value, axis=1)\n",
    "# ---------------------------\n",
    "\n",
    "Y0 = np.array([[2, 0]])  # De la forme (k,2)\n",
    "\n",
    "L_System = m*l**2/2*theta_d**2+sp.cos(theta)*l*m*g\n",
    "\n",
    "Acc_func,_ = Lagrangian_to_Acc_func(L_System, Symb, t, Substitution, fluid_f=[-0.02])\n",
    "\n",
    "Dynamics_system = Dynamics_f_extf(Acc_func)\n",
    "\n",
    "EnvConfig = {\n",
    "    \"coord_numb\": CoordNumb,\n",
    "    \"target\":np.array([np.pi,0]),\n",
    "    \"dynamics_function_h\":Dynamics_system,\n",
    "    \"h\":0.02\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Know we can do the training for our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "AlgorithmConfig.resources() got an unexpected keyword argument 'num_cpus_per_env_runner'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mPPOConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Env class to use (here: our gym.Env sub-class from above).\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMyFunctionEnv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEnvConfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresources\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_cpus_per_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_gpus_per_env_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Parallelize environment rollouts.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39menv_runners(num_env_runners\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mtraining(lr\u001b[38;5;241m=\u001b[39mtune\u001b[38;5;241m.\u001b[39mgrid_search([\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.0001\u001b[39m, \u001b[38;5;241m0.0005\u001b[39m]),gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,entropy_coeff\u001b[38;5;241m=\u001b[39mtune\u001b[38;5;241m.\u001b[39mgrid_search([\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.005\u001b[39m, \u001b[38;5;241m0.0005\u001b[39m]))\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m tuner \u001b[38;5;241m=\u001b[39m tune\u001b[38;5;241m.\u001b[39mTuner(\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPPO\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     run_config\u001b[38;5;241m=\u001b[39mtrain\u001b[38;5;241m.\u001b[39mRunConfig(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     param_space\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     22\u001b[0m results \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mfit()\n",
      "\u001b[0;31mTypeError\u001b[0m: AlgorithmConfig.resources() got an unexpected keyword argument 'num_cpus_per_env_runner'"
     ]
    }
   ],
   "source": [
    "config = (\n",
    "    PPOConfig().environment(\n",
    "        # Env class to use (here: our gym.Env sub-class from above).\n",
    "        env=MyFunctionEnv,\n",
    "        env_config=EnvConfig,\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .env_runner(num_cpus_per_env_runner=1, num_gpus_per_env_runner=1 / 16)\n",
    "    # Parallelize environment rollouts.\n",
    "    .env_runners(num_env_runners=10)\n",
    "    .training(lr=tune.grid_search([0.001, 0.0001, 0.0005]),gamma=0.9,entropy_coeff=tune.grid_search([0.001, 0.005, 0.0005]))\n",
    ")\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    \"PPO\",\n",
    "    run_config=train.RunConfig(\n",
    "        stop={\"training_iteration\": 4},\n",
    "    ),\n",
    "    param_space=config,\n",
    ")\n",
    "\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(\n",
      "  metrics={'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'default_policy': {'learner_stats': {'allreduce_latency': 0.0, 'grad_gnorm': 1.5296250750941616, 'cur_kl_coeff': 0.4500000000000001, 'cur_lr': 0.0005000000000000002, 'total_loss': 9.751829777994464, 'policy_loss': -0.04646840618863221, 'vf_loss': 9.786792869978054, 'vf_explained_var': -0.2283534578097764, 'kl': 0.025567411180893562, 'entropy': 1.2679376895709704, 'entropy_coeff': 0.0}, 'model': {}, 'custom_metrics': {}, 'num_agent_steps_trained': 128.0, 'num_grad_updates_lifetime': 4185.5, 'diff_num_grad_updates_vs_sampler_policy': 464.5}}, 'num_env_steps_sampled': 20000, 'num_env_steps_trained': 20000, 'num_agent_steps_sampled': 20000, 'num_agent_steps_trained': 20000}, 'env_runners': {'episode_reward_max': 565.9971998665721, 'episode_reward_min': -1739.6290388900309, 'episode_reward_mean': -497.16871318345716, 'episode_len_mean': 60.21, 'episode_media': {}, 'episodes_timesteps_total': 6021, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-496.723551044082, -643.9508696765059, -489.73071675043263, -502.8915951518728, -670.2972780902986, -588.8367825434294, -517.1394095165385, -494.69378583417574, -612.5258158723072, -421.55220345831816, -1242.4992670502015, -451.5023081879536, -600.5551433024845, -737.4298517890516, -677.8531617106697, -640.9718201113453, -981.1996313149098, -236.84266931894672, -1203.0047897040317, 410.53692642201577, -347.7909458305899, -390.8035482716262, -431.31700761322656, -875.8900506032419, -1739.6290388900309, -413.275386944202, -597.0601160039129, -338.46899394311646, -298.4178122907828, -276.8315908591308, -417.37595895438926, -963.3538505213039, -504.35943783049333, -656.3190589239828, -286.16620323655286, -582.6074754406411, -909.606247324705, -494.761333366314, -246.08378855053024, -499.4900548227527, -438.20574086251474, -450.61692409911564, -488.9665291132142, -405.522460695344, -375.77144550602776, -662.4653144102472, -522.6083451925134, -478.31277897733577, -407.39552883116346, -565.7311935831534, -464.17311687162544, -249.0401563116221, -684.660655877099, -577.3034250130506, -397.9555469462335, -373.25612773673834, -1017.6005739870357, -481.7564110127315, -408.3602635290436, -406.02495006527806, -427.47252302150895, -400.9447967889528, -564.3606026744732, -297.2838552327048, -324.72260361315796, -359.11362206158816, -474.1437864516358, -402.7550478386893, -389.1458840477336, -500.5412379459793, -544.5841768793787, -383.5051365578764, -399.90576598603656, -350.4645772062236, -616.766478602854, -447.0969549601902, -358.1917889400502, -485.63570232084317, -558.8098466951362, -268.67722938964033, -504.1097246826036, -366.928049724998, -805.0379246409091, -439.5235301185473, -371.6177378645322, -533.7921377576648, -344.00693796697084, -444.1942158238759, -323.7248870813316, -357.67897764222505, -465.27026551395426, -283.85225574505637, 565.9971998665721, -446.976698963731, -636.358816333713, -599.5861470775137, -475.7661408092658, -557.7610254520258, -310.31298842282695, -537.2493265277279], 'episode_lengths': [61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 38, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 61, 5, 61, 61, 61, 61, 61, 61, 61]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 0.638099778579343, 'mean_inference_ms': 10.330169172835635, 'mean_action_processing_ms': 0.3540436799630686, 'mean_env_wait_ms': 0.5897748942242714, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.014229297637939453, 'StateBufferConnector_ms': 0.00760650634765625, 'ViewRequirementAgentConnector_ms': 0.23031282424926758}, 'num_episodes': 62, 'episode_return_max': 565.9971998665721, 'episode_return_min': -1739.6290388900309, 'episode_return_mean': -497.16871318345716, 'episodes_this_iter': 62}, 'num_healthy_workers': 10, 'num_in_flight_async_sample_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 20000, 'num_agent_steps_trained': 20000, 'num_env_steps_sampled': 20000, 'num_env_steps_trained': 20000, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 4000, 'num_env_steps_sampled_throughput_per_sec': 370.93868876088817, 'num_env_steps_trained_throughput_per_sec': 370.93868876088817, 'num_env_steps_sampled_lifetime': 20000, 'num_agent_steps_sampled_lifetime': 20000, 'num_steps_trained_this_iter': 4000, 'agent_timesteps_total': 20000, 'timers': {'training_iteration_time_ms': 10831.663, 'restore_workers_time_ms': 0.024, 'training_step_time_ms': 10831.601, 'sample_time_ms': 4928.125, 'load_time_ms': 0.357, 'load_throughput': 11205728.026, 'learn_time_ms': 5875.98, 'learn_throughput': 680.738, 'synch_weights_time_ms': 26.315}, 'counters': {'num_env_steps_sampled': 20000, 'num_env_steps_trained': 20000, 'num_agent_steps_sampled': 20000, 'num_agent_steps_trained': 20000}, 'perf': {'cpu_util_percent': 34.59333333333334, 'ram_util_percent': 90.29999999999998}},\n",
      "  path='/home/eymeric/ray_results/PPO_2024-07-04_12-14-50/PPO_MyFunctionEnv_9555f_00006_6_gamma=0.9000,lr=0.0005_2024-07-04_12-14-54',\n",
      "  filesystem='local',\n",
      "  checkpoint=Checkpoint(filesystem=local, path=/home/eymeric/ray_results/PPO_2024-07-04_12-14-50/PPO_MyFunctionEnv_9555f_00006_6_gamma=0.9000,lr=0.0005_2024-07-04_12-14-54/checkpoint_000000)\n",
      ")\n",
      "Checkpoint(filesystem=local, path=/home/eymeric/ray_results/PPO_2024-07-04_12-14-50/PPO_MyFunctionEnv_9555f_00006_6_gamma=0.9000,lr=0.0005_2024-07-04_12-14-54/checkpoint_000000)\n"
     ]
    }
   ],
   "source": [
    "# Get the best result based on a particular metric.\n",
    "best_result = results.get_best_result(\n",
    "    metric=\"env_runners/episode_return_mean\", mode=\"max\"\n",
    ")\n",
    "\n",
    "# Get the best checkpoint corresponding to the best result.\n",
    "best_checkpoint = best_result.checkpoint\n",
    "\n",
    "pprint.pp(best_result)\n",
    "pprint.pp(best_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can do more training for our policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 12:31:22,226\tWARNING deprecation.py:50 -- DeprecationWarning: `_enable_new_api_stack` has been deprecated. Use `AlgorithmConfig._enable_new_api_stack` instead. This will raise an error in the future!\n",
      "2024-07-04 12:31:22,227\tWARNING deprecation.py:50 -- DeprecationWarning: `AlgorithmConfig.num_cpus_per_worker` has been deprecated. Use `AlgorithmConfig.num_cpus_per_env_runner` instead. This will raise an error in the future!\n",
      "2024-07-04 12:31:22,228\tWARNING deprecation.py:50 -- DeprecationWarning: `AlgorithmConfig.num_gpus_per_worker` has been deprecated. Use `AlgorithmConfig.num_gpus_per_env_runner` instead. This will raise an error in the future!\n",
      "2024-07-04 12:31:22,229\tWARNING deprecation.py:50 -- DeprecationWarning: `AlgorithmConfig.num_learner_workers` has been deprecated. Use `AlgorithmConfig.num_learners` instead. This will raise an error in the future!\n",
      "2024-07-04 12:31:22,229\tWARNING deprecation.py:50 -- DeprecationWarning: `AlgorithmConfig.num_cpus_per_learner_worker` has been deprecated. Use `AlgorithmConfig.num_cpus_per_learner` instead. This will raise an error in the future!\n",
      "2024-07-04 12:31:22,230\tWARNING deprecation.py:50 -- DeprecationWarning: `AlgorithmConfig.num_gpus_per_learner_worker` has been deprecated. Use `AlgorithmConfig.num_gpus_per_learner` instead. This will raise an error in the future!\n",
      "/home/eymeric/Lagrangian_sindy/.venv/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:516: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/eymeric/Lagrangian_sindy/.venv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/eymeric/Lagrangian_sindy/.venv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/eymeric/Lagrangian_sindy/.venv/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-07-04 12:31:32,582\tINFO trainable.py:161 -- Trainable.setup took 10.290 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-07-04 12:31:32,585\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "2024-07-04 12:31:37,919\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0; avg. return=-296.9169161957483\n",
      "Iter: 1; avg. return=-283.9138503574072\n",
      "Iter: 2; avg. return=-230.25194964932336\n",
      "Iter: 3; avg. return=-154.0041364411379\n",
      "Iter: 4; avg. return=-100.64838800039341\n",
      "Iter: 5; avg. return=-85.47237617110115\n",
      "Iter: 6; avg. return=-56.08924529654882\n",
      "Iter: 7; avg. return=-55.48592151975458\n",
      "Iter: 8; avg. return=-44.75891582197993\n",
      "Iter: 9; avg. return=-16.918772040000707\n",
      "Iter: 10; avg. return=-14.441446887020623\n",
      "Iter: 11; avg. return=-25.544092289124237\n",
      "Iter: 12; avg. return=-30.356257978641743\n",
      "Iter: 13; avg. return=24.392487430393775\n",
      "Iter: 14; avg. return=30.98715258886906\n",
      "Iter: 15; avg. return=8.9407188190476\n",
      "Iter: 16; avg. return=36.29991166643517\n",
      "Iter: 17; avg. return=-6.2262621219443695\n",
      "Iter: 18; avg. return=0.22359168473684435\n",
      "Iter: 19; avg. return=20.612284011646256\n",
      "Iter: 20; avg. return=59.62594613783131\n",
      "Iter: 21; avg. return=48.49687209477452\n",
      "Iter: 22; avg. return=55.60378172614796\n",
      "Iter: 23; avg. return=55.982820766046395\n",
      "Iter: 24; avg. return=68.07553569772188\n",
      "Iter: 25; avg. return=58.00767781817618\n",
      "Iter: 26; avg. return=83.3391071037057\n",
      "Iter: 27; avg. return=73.07966448486557\n",
      "Iter: 28; avg. return=127.83780223089596\n",
      "Iter: 29; avg. return=133.02679875492015\n",
      "Iter: 30; avg. return=122.41408474813947\n",
      "Iter: 31; avg. return=147.758491303298\n",
      "Iter: 32; avg. return=116.41638232248496\n",
      "Iter: 33; avg. return=86.06817627135389\n",
      "Iter: 34; avg. return=110.43675262517577\n",
      "Iter: 35; avg. return=159.29573515746452\n",
      "Iter: 36; avg. return=116.80606901796845\n",
      "Iter: 37; avg. return=153.0612132885347\n",
      "Iter: 38; avg. return=192.69402035514145\n",
      "Iter: 39; avg. return=242.01580594492216\n",
      "An Algorithm checkpoint has been created inside directory: '/tmp/tmpoonj8141'.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "my_new_ppo = Algorithm.from_checkpoint(\"/home/eymeric/ray_results/PPO_2024-07-04_12-14-50/PPO_MyFunctionEnv_9555f_00006_6_gamma=0.9000,lr=0.0005_2024-07-04_12-14-54/checkpoint_000000\")\n",
    "\n",
    "# Continue training\n",
    "for i in range(40):\n",
    "    results = my_new_ppo.train()\n",
    "    print(f\"Iter: {i}; avg. return={results['env_runners']['episode_return_mean']}\")\n",
    "\n",
    "save_result = my_new_ppo.save()\n",
    "path_to_checkpoint = save_result.checkpoint.path\n",
    "print(\n",
    "    \"An Algorithm checkpoint has been created inside directory: \"\n",
    "    f\"'{path_to_checkpoint}'.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "stop = False\n",
    "Environment = MyFunctionEnv(EnvConfig)\n",
    "\n",
    "my_new_ppo = Algorithm.from_checkpoint(\"/tmp/tmpoonj8141\")\n",
    "\n",
    "\n",
    "while not stop:\n",
    "\n",
    "    action = my_new_ppo.compute_single_action(Environment.state)\n",
    "\n",
    "    state, reward, stop, truncated,_ = Environment.step(action)\n",
    "\n",
    "    print(state, reward, action, stop, truncated)\n",
    "\n",
    "    Environment.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
